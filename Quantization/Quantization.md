> Contained Binarized Neural Networkds

- [Forward and Backward Information Retention for Accurate Binary Neural Networks](https://arxiv.org/abs/1909.10788) | [code](https://github.com/htqin/IR-Net)  [arXiv  '19]
- [And the bit goes down: Revisiting the quantization of neural networks](https://arxiv.org/abs/1907.05686)   [arXiv  '19]
- [CompactNet: Platform-Aware Automatic Optimization for Convolutional Neural Networks](https://arxiv.org/abs/1905.11669) [arXiv  '19]
- [FLightNNs](https://arxiv.org/abs/1904.02835) [arXiv  '19]
- [Towards Efficient Model Compression via Learned Global Ranking](https://arxiv.org/abs/1904.12368) | [code](https://github.com/cmu-enyac/LeGR) [arXiv  '19]
- [Training and Inference with Integers in Deep Neural Networks](https://openreview.net/forum?id=HJGXzmspb) [ICLR '18]
- [Training deep neural networks with 8-bit floating point numbers](https://arxiv.org/abs/1812.08011) [arXiv  '18]
- [Training deep neural networks with low precision multiplications](https://arxiv.org/abs/1412.7024) [arXiv  '14]
- [LightNN: Filling the Gap between Conventional Deep Neural Networks and Binarized Networks](https://arxiv.org/abs/1802.02178)  [arXiv  '18]
- [Minimum Energy Quantized Neural Networks](https://arxiv.org/abs/1711.00215)  [arXiv  '17]
- [ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks](https://arxiv.org/abs/1706.02393)  [arXiv  '17]
- [Loss-aware Binarization of Deep Networks](https://arxiv.org/abs/1611.01600) [ICLR '17]
- [The ZipML Framework for Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning](https://arxiv.org/abs/1611.05402) [ICML '17]
- [Deep Learning with Low Precision by Half-wave Gaussian Quantization](https://arxiv.org/abs/1702.00953)  [arXiv  '17]
- [Towards the Limit of Network Quantization](https://openreview.net/forum?id=rJ8uNptgl)  [arXiv  '17]
- [Binarized Neural Networks](https://arxiv.org/abs/1602.02830) [arXiv  '16]
- [XNOR-Net](https://arxiv.org/abs/1603.05279) [arXiv  '16]
- [DoReFa-Net](https://arxiv.org/abs/1606.06160) [arXiv  '16]
- [Quantized Convolutional Neural Networks for Mobile Devices](https://arxiv.org/abs/1512.06473) [CVPR  '16]
- [Towards the Limit of Network Quantization](https://arxiv.org/abs/1612.01543)  [arXiv  '16]
- [Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations](https://arxiv.org/abs/1609.07061)  [arXiv  '16]
- [Trained Ternary Quantization](https://arxiv.org/abs/1612.01064)  [arXiv  '16]
- [The ZipML Framework for Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning](https://arxiv.org/abs/1611.05402)  [arXiv  '16]
- [Loss-aware Binarization of Deep Networks](https://arxiv.org/abs/1611.01600)  [arXiv  '16]
- [Quantize weights and activations in Recurrent Neural Networks](https://arxiv.org/abs/1611.10176)   [arXiv  '16]
- [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)  [arXiv  '15]
- [Compressing Neural Networks with the Hashing Trick](https://arxiv.org/abs/1504.04788)   [arXiv  '15]
- [Quantized Convolutional Neural Networks for Mobile Devices ](https://arxiv.org/abs/1512.06473)  [arXiv  '15]
- [Fixed-Point Performance Analysis of Recurrent Neural Networks](https://arxiv.org/abs/1512.01322)   [arXiv  '15]
- [Compressing Deep Convolutional Networks using Vector Quantization](https://arxiv.org/abs/1412.6115) [arXiv  '14]