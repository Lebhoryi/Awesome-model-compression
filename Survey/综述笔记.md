[TOC]

> 阅读综述时做的笔记

# 1. 神经网络模型压缩方法综述_张弛

其中，剪枝的方法是通过搜索模型中冗余的参数并将之修剪掉；量化的方法是通过减小每个权重的比特数来压缩原始网络的存储空间；而低秩分解的方法是将卷积神经网络中的卷积核看成一个矩阵或张量，通过对其进行低秩分解来消除模型中的冗余部分。

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201015144321.png)

# 2. 深度神经网络模型压缩综述_耿丽丽

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201015150102.png)

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201015150114.png)

浅层压缩： 知识蒸馏和剪枝不会改变网络的基本结构。该篇将剪枝划分为了四个方面：weight、channel、filter、neuron

- **[最早]** Advances in Neural Information Processing Systems, 1990,
通过删除网络中不重要的权重，使得神经网络能够更好地泛化，提高学习速率，达到压缩网络尺寸的目的。
- Deep compression: compressing deep neural networks with pruning, trained quantization and Huffman coding
**2016年ICLR的最佳论文**

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201015152559.png)

## 1. Weight

流程：

- 预训练
- 通过裁剪阈值或者损失函数增加正则化
- 微调
- 迭代训练

缺点：

权重裁剪往往会造成网络模型的不规则稀疏性，不能充分降低卷积层中的计算成本，因此就有了 Filter 剪枝

## 2. Channel

将整个通道删除，与之相关的神经元也会被删除，裁剪力度大，精度损失也大。**粗粒度剪枝**

优点：

- 它不产生稀疏矩阵，不需要特殊的软件或硬件来针对所得到的模型进行计算

- 在推理阶段不需要巨大的磁盘存储和内存运行时间

代表论文：遗传算法、lasso 回归

## 3. Filter

优点： 对卷积核的低秩分解则可以保证网络模型**稀疏性**的同时简化矩阵计算，降低计算成本

代表论文：裁剪滤波器及其连接的特征映射、Taylor

## 4. Neuron

神经元部分为零，对该部分做裁剪。

代表论文：Network trimming等

## 5. 评价指标

- 参数减少量和压缩率是评价模型参数量的绝对值和相对值 

- 压缩率为原参数量与压缩后参数量的比值 
- 数据集
- Top-k
- Flops减少量

# 3. 深度神经网络模型压缩综述_李江昀

模型压缩方法：pruning、parameters sharing、quantization、decomposition、distilling、compact network

pruning 分为：

- weight

  属于非结构化剪枝，会导致稀疏分布，

  代表论文：

  1. 最早: LeCun 的  Optimal brain damage,  优化时基于对角假设、极值假 设和二次假设利用二阶导数近似参数显著性移除网 络中不重要的权重来提高网络精度和泛化能力．1989
  2. 接着，Optimal brain surgeon and general network pruning 和 Second order derivatives for network pruning: optimal brain surgeon 1993，基于手术恢复权重更新步骤的最优脑手术
  3. Learning both weights and connections for efficient neural network  保留重要连接 Han 2015
  4. 三种方法混合，Han 2015年

  缺点：

  1. 仅仅对部分参数进行剪枝并不能显著降低计算量与参数量;
  2. 剪枝部分参数将会得到稀疏网络，而目前对稀疏操作加速支持的库非常有限; 
  3. 得到的稀疏数据结构将会需要额外存储开销尤其对于低精度权重

- intra kernel weight

  针对上述非结构化剪枝的问题，提出了结构化剪枝

  代表论文： 

  ​	结构化剪枝开山之作、首篇引入正则化、dynamic network surgery

- kernel / channel /feature map

   卷积核剪枝与通道剪枝都属于粗粒度剪枝，剪枝后模型使用现有硬件与计算库即可进行训练，并且最终可以获得更高压缩率与更短计算时间．

  代表论文：

  ​	filter pruning 开山之作、ThiNet、network trimming、Soft filter pruning、 lasso 回归、 判别力驱动损失函数、强化学习

- layer

# 4. 闲话模型压缩之网络剪枝（Network Pruning）篇

早先的属于非结构化剪枝，裁剪对象为单个神经元。

如果对kernel进行非结构化剪枝，则得到的kernel是稀疏的，即中间有很多元素为0的矩阵。除非下层的硬件和计算库对其有比较好的支持，pruning后版本很难获得实质的性能提升。稀疏矩阵无法利用现有成熟的BLAS库获得额外性能收益。

# 5. 2005.04275

划分为两个类别：data-agnostic \ data-driven

OBD: 二阶 Hessian 矩阵，计算量仍在。

早先的方法虽然在有效的减少了模型大小和计算，但是减少FLOPs对推理加速没有明显的联系。因此提出了结构化剪枝。

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201016102743.png)

设置预定义阈值的方式有三个缺点：

1. 和稀疏没直接关系
2. 不同的层具有不同的灵敏度
3. 一次性阈值可能剪掉重要的信息导致精度下降

因此，有人提出用贝叶斯来获取每层的最佳阈值、引入l2 范数来自动卷积核剪枝等方法。

使用正则化。

l1 \ l2 范数的方法可能会导致不稳定和次优解。解决：PAPG、ADMM、动态、less is more。

强化学习：NAS

三个优化方向：

1. 有效的迭代过程
2. mask
3. 在训练过程中剪枝

评价指标：

1. Accuracy reduction
2. Size reduction
3. Time reduction

剪枝的优点：

1. 减少模型大小
2. 加速推理
3. 普遍适用的方法

缺点：

1. 训练时间长，解决：动态训练
2. 额外的超参微调
3. 小但不快
4. 缺少一个公认的基准线

