- [Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey](https://ieeexplore.ieee.org/abstract/document/9043731) [IEEE  '20]

  > compact model \ tensor decomposition \ data quantization \ network sparsifification(pruning)

- [Pruning Algorithms to Accelerate Convolutional Neural Networks for Edge Applications: A Survey](https://arxiv.org/abs/2005.04275) | [arXiv  '20]

- [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525)

- Binary neural networks: A survey

- Sparse low rank factorization for deep neural network compression

- Taxonomy and evaluation of structured compression of convolutional neural networks.  2019

- [A Survey of Methods for Low-Power Deep Learning and Computer Vision](https://arxiv.org/abs/2003.11066) [arXiv  '20]

- [Recent Advances in Efficient Computation of Deep Convolutional Neural Networks](https://arxiv.org/abs/1802.00939) [arXiv  '18]

  > network pruning, low-rank approximation, network quantization, teacher-student networks, compact network design and hardware accelerators

- Survey of model compression method for neural networks   [arXiv  '18]

- Survey of neural network model compression method   [arXiv  '18]

- [A Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/abs/1710.09282) [arXiv  '17]

- [Model compression as constrained optimization, with application to neural nets. Part I: general framework](https://arxiv.org/abs/1707.01209) [arXiv  '17]

- [Model compression as constrained optimization, with application to neural nets. Part II: quantization ](https://arxiv.org/abs/1707.04319) [arXiv  '17]
